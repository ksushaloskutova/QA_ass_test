from telegram import Update
import os
from telegram.ext import ApplicationBuilder, ContextTypes, MessageHandler, filters
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain.llms import HuggingFacePipeline

from qa_engine import QAEngine

# –ó–∞–≥—Ä—É–∂–∞–µ–º LLM
tokenizer = AutoTokenizer.from_pretrained("sberbank-ai/rugpt3small_based_on_gpt2")
model = AutoModelForCausalLM.from_pretrained("sberbank-ai/rugpt3small_based_on_gpt2")
pipe = pipeline("text-generation", model=model, tokenizer=tokenizer, max_new_tokens=150)
llm = HuggingFacePipeline(pipeline=pipe)

qa_engine = QAEngine()

async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_input = update.message.text
    response = qa_engine.answer(user_input)
    await update.message.reply_text(response)

def run_telegram_bot():
    app = ApplicationBuilder().token(os.getenv("TELEGRAM_TOKEN")).build()
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))
    print("ü§ñ –ê–≥–µ–Ω—Ç-–±–æ—Ç –∑–∞–ø—É—â–µ–Ω!")
    app.run_polling()
