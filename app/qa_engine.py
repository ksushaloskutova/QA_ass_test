from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_community.llms import HuggingFacePipeline
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline

import config
import html_parser


class QAEngine:
    """Ð”Ð²Ð° Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ñ‹Ñ… QA-Ð´Ð²Ð¸Ð¶ÐºÐ° Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼ AI Ð¸ AI Product."""

    def __init__(self) -> None:
        print("ðŸ”„ Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÑŽ Ð¸ Ð¾Ð±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÑŽ ÑƒÑ‡ÐµÐ±Ð½Ñ‹Ðµ Ð¿Ð»Ð°Ð½Ñ‹...")

        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=config.CHUNK_SIZE,
            chunk_overlap=config.CHUNK_OVERLAP,
        )

        print("ðŸ“ Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¾Ð²...")
        embedding_model = HuggingFaceEmbeddings(
            model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        )

        print("ðŸ§  Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° ÑÐ·Ñ‹ÐºÐ¾Ð²Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸ cointegrated/rut5-base...")
        tokenizer = AutoTokenizer.from_pretrained("cointegrated/rut5-small")
        model = AutoModelForSeq2SeqLM.from_pretrained("cointegrated/rut5-small")
        pipe = pipeline(
            "text2text-generation",
            model=model,
            tokenizer=tokenizer,
            max_length=512,
            do_sample=False,
        )
        self.llm = HuggingFacePipeline(pipeline=pipe)

        self.prompt = PromptTemplate(
            template=(
                "ÐšÐ¾Ð½Ñ‚ÐµÐºÑÑ‚:\n{context}\n\n"
                "Ð’Ð¾Ð¿Ñ€Ð¾Ñ:\n{question}\n\n"
                "ÐžÑ‚Ð²ÐµÑ‚:"
            ),
            input_variables=["context", "question"],
        )

        # Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ð¼ Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ñ‹Ðµ QA-Ñ†ÐµÐ¿Ð¾Ñ‡ÐºÐ¸ Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð¹ Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ñ‹
        self.qa_chains = {}
        for program, url in config.PDF_URLS.items():
            text = html_parser.fetch_pdf_text(url)
            if not text:
                raise ValueError(f"âŒ ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð·Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ ÑƒÑ‡ÐµÐ±Ð½Ñ‹Ð¹ Ð¿Ð»Ð°Ð½: {url}")

            docs = text_splitter.create_documents([text])
            db = FAISS.from_documents(docs, embedding_model)
            retriever = db.as_retriever(search_kwargs={"k": 3})
            chain = RetrievalQA.from_chain_type(
                llm=self.llm,
                retriever=retriever,
                chain_type="refine",
                chain_type_kwargs={"prompt": self.prompt},
            )
            self.qa_chains[program] = chain

    def _run_chain(self, query: str, program: str) -> str:
        chain = self.qa_chains.get(program)
        if not chain:
            raise ValueError("ÐÐµÐ¸Ð·Ð²ÐµÑÑ‚Ð½Ð°Ñ Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ð°")
        result = chain.invoke({"query": query})
        if isinstance(result, dict):
            return result.get("result", "")
        return result

    def answer(self, query: str, program: str) -> str:
        """ÐžÑ‚Ð²ÐµÑ‚ Ð´Ð»Ñ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ð¾Ð¹ Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ñ‹."""
        return self._run_chain(query, program)

    def compare(self, query: str) -> str:
        """Ð¡Ñ€Ð°Ð²Ð½ÐµÐ½Ð¸Ðµ Ð¾Ñ‚Ð²ÐµÑ‚Ð¾Ð² Ð¿Ð¾ Ð¾Ð±ÐµÐ¸Ð¼ Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ð°Ð¼."""
        ai_answer = self._run_chain(query, "ai")
        ai_prod_answer = self._run_chain(query, "ai_product")
        return (
            f"ÐŸÑ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ð° AI:\n{ai_answer}\n\n"
            f"ÐŸÑ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ð° AI Product:\n{ai_prod_answer}"
        )


    def answer(self, query: str) -> str:
        result = self.qa_chain.invoke({"query": query})
        return result["result"]

