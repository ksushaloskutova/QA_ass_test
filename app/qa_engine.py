from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_community.llms import HuggingFacePipeline
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
import config, html_parser


class QAEngine:
    def __init__(self):
        print("ðŸ”„ Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÑŽ Ð¸ Ð¾Ð±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÑŽ ÑƒÑ‡ÐµÐ±Ð½Ñ‹Ðµ Ð¿Ð»Ð°Ð½Ñ‹...")

        documents = []
        self.course_data = []
        for url in config.PDF_URLS:
            text = html_parser.fetch_pdf_text(url)
            if text:
                documents.append(text)
                parsed = html_parser.parse_course_data(text)
                if parsed:
                    self.course_data.extend(parsed)

        if not documents:
            raise ValueError("âŒ ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð·Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Ð½Ð¸ Ð¾Ð´Ð½Ð¾Ð³Ð¾ ÑƒÑ‡ÐµÐ±Ð½Ð¾Ð³Ð¾ Ð¿Ð»Ð°Ð½Ð°.")

        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=config.CHUNK_SIZE,
            chunk_overlap=config.CHUNK_OVERLAP
        )
        docs = text_splitter.create_documents(documents)

        print("ðŸ“ Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¾Ð²...")
        embedding_model = HuggingFaceEmbeddings(
            model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        )
        self.db = FAISS.from_documents(docs, embedding_model)

        print("ðŸ§  Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° ÑÐ·Ñ‹ÐºÐ¾Ð²Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸ cointegrated/rut5-base...")
        tokenizer = AutoTokenizer.from_pretrained("cointegrated/rut5-small")
        model = AutoModelForSeq2SeqLM.from_pretrained("cointegrated/rut5-small")

        pipe = pipeline(
            "text2text-generation",
            model=model,
            tokenizer=tokenizer,
            max_length=512,
            do_sample=False
        )
        self.llm = HuggingFacePipeline(pipeline=pipe)

        self.retriever = self.db.as_retriever(search_kwargs={"k": 3})

        self.prompt = PromptTemplate(
            template=(
                "ÐšÐ¾Ð½Ñ‚ÐµÐºÑÑ‚:\n{context}\n\n"
                "Ð’Ð¾Ð¿Ñ€Ð¾Ñ:\n{question}\n\n"
                "ÐžÑ‚Ð²ÐµÑ‚:"
            ),
            input_variables=["context", "question"]
        )

        self.qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            retriever=self.retriever,
            chain_type="refine",
            chain_type_kwargs={"prompt": self.prompt}
        )

    def answer(self, query: str) -> str:
        result = self.qa_chain.invoke({"query": query})
        return result["result"]
